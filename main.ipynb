{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (23573, 155)\n",
      "Column Names: Index(['study_id', 'eureka_id', 'day', 'act_in_vehicle_ep_0',\n",
      "       'act_in_vehicle_ep_1', 'act_in_vehicle_ep_2', 'act_in_vehicle_ep_3',\n",
      "       'act_in_vehicle_ep_4', 'act_on_bike_ep_0', 'act_on_bike_ep_1',\n",
      "       ...\n",
      "       'unlock_duration_ep_0', 'unlock_duration_ep_1', 'unlock_duration_ep_2',\n",
      "       'unlock_duration_ep_3', 'unlock_duration_ep_4', 'unlock_num_ep_0',\n",
      "       'unlock_num_ep_1', 'unlock_num_ep_2', 'unlock_num_ep_3',\n",
      "       'unlock_num_ep_4'],\n",
      "      dtype='object', length=155)\n",
      "   study_id eureka_id       day  act_in_vehicle_ep_0  act_in_vehicle_ep_1  \\\n",
      "0        -1      u004  20150122                    0                    0   \n",
      "1        -1      u004  20150123                    0                    0   \n",
      "2        -1      u004  20150124                    0                    0   \n",
      "3        -1      u004  20150125                    0                    0   \n",
      "4        -1      u004  20150126                    0                    0   \n",
      "\n",
      "   act_in_vehicle_ep_2  act_in_vehicle_ep_3  act_in_vehicle_ep_4  \\\n",
      "0                    0                    0                    0   \n",
      "1                    0                    0                    0   \n",
      "2                    0                    0                    0   \n",
      "3                    0                    0                    0   \n",
      "4                    0                    0                    0   \n",
      "\n",
      "   act_on_bike_ep_0  act_on_bike_ep_1  ...  unlock_duration_ep_0  \\\n",
      "0                 0                 0  ...                   0.0   \n",
      "1                 0                 0  ...                   0.0   \n",
      "2                 0                 0  ...                   0.0   \n",
      "3                 0                 0  ...                   0.0   \n",
      "4                 0                 0  ...                   0.0   \n",
      "\n",
      "   unlock_duration_ep_1  unlock_duration_ep_2  unlock_duration_ep_3  \\\n",
      "0                   0.0                   0.0                   0.0   \n",
      "1                   0.0                   0.0                   0.0   \n",
      "2                   0.0                   0.0                   0.0   \n",
      "3                   0.0                   0.0                   0.0   \n",
      "4                   0.0                   0.0                   0.0   \n",
      "\n",
      "   unlock_duration_ep_4  unlock_num_ep_0  unlock_num_ep_1  unlock_num_ep_2  \\\n",
      "0                   0.0                0                0                0   \n",
      "1                   0.0                0                0                0   \n",
      "2                   0.0                0                0                0   \n",
      "3                   0.0                0                0                0   \n",
      "4                   0.0                0                0                0   \n",
      "\n",
      "   unlock_num_ep_3  unlock_num_ep_4  \n",
      "0                0                0  \n",
      "1                0                0  \n",
      "2                0                0  \n",
      "3                0                0  \n",
      "4                0                0  \n",
      "\n",
      "[5 rows x 155 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'CrossCheck_Daily_Data.xlsx'  \n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"Column Names:\", data.columns)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the eps to make it one full day\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "new_columns = []\n",
    "processed_base_columns = set()  \n",
    "columns_to_drop = []  \n",
    "\n",
    "\n",
    "epochs = ['ep_0', 'ep_1', 'ep_2', 'ep_3']  #only taking the first four eps as they indicates the whole day\n",
    "\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if any(epoch in col for epoch in epochs):  # to see if the coulmn has ep_ in it \n",
    "        \n",
    "        #removing ep for the bas coulmn name\n",
    "        epoch_num = [epoch for epoch in epochs if epoch in col][0]\n",
    "        base_col_name = col.replace(f'_{epoch_num}', '') \n",
    "\n",
    "        # Check if the coulmn has been aggregated before and not aggregate ep_4 \n",
    "        if base_col_name not in processed_base_columns:\n",
    "            columns_to_sum = [col for col in data.columns if base_col_name in col and 'ep_4' not in col]\n",
    "            aggregated_col = data[columns_to_sum].sum(axis=1)\n",
    "            new_columns.append(aggregated_col.rename(base_col_name + '_sum'))\n",
    "            \n",
    "\n",
    "            columns_to_drop.extend([col for col in columns_to_sum if 'ep_4' not in col]) \n",
    "\n",
    "            # Mark the base column as processed\n",
    "            processed_base_columns.add(base_col_name)\n",
    "\n",
    "# add the new coulmns to the data \n",
    "if new_columns:\n",
    "    data = pd.concat([data] + new_columns, axis=1)\n",
    "\n",
    "#drop the original coulmns \n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "#data.to_csv(\"processed_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Identify rows with recorded EMA scores\n",
    "data['is_target'] = data['ema_score'].notna()\n",
    "\n",
    "# Find the indices where EMA scores are available\n",
    "ema_indices = data[data['is_target']].index.tolist()\n",
    "\n",
    "# List to store new dataset rows\n",
    "new_data = []\n",
    "\n",
    "# Extract feature columns (excluding 'ema_score', 'study_id', 'eureka_id', and 'day')\n",
    "feature_cols = [col for col in data.columns if col not in ['ema_score', 'study_id', 'eureka_id', 'day', 'is_target']]\n",
    "\n",
    "# Iterate over each EMA segment\n",
    "for i in range(len(ema_indices) - 1):\n",
    "    start_idx = ema_indices[i]  # Start from last recorded EMA\n",
    "    end_idx = ema_indices[i + 1]  # Stop at next recorded EMA\n",
    "\n",
    "    # Get segment data\n",
    "    segment = data.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Compute average of feature columns\n",
    "    avg_features = segment[feature_cols].mean().to_dict()\n",
    "\n",
    "    # Assign the next recorded EMA score as the target\n",
    "    avg_features['ema_score'] = data.loc[end_idx, 'ema_score']\n",
    "\n",
    "    # Store metadata\n",
    "    avg_features['study_id'] = data.loc[start_idx, 'study_id']  # Keep the participant ID\n",
    "    avg_features['start_day'] = data.loc[start_idx, 'day']  # First day in the segment\n",
    "    avg_features['end_day'] = data.loc[end_idx, 'day']  # Last day in the segment\n",
    "\n",
    "    # Store the result\n",
    "    new_data.append(avg_features)\n",
    "\n",
    "# Convert new dataset to DataFrame\n",
    "data = pd.DataFrame(new_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   act_in_vehicle_ep_4  act_on_bike_ep_4  act_on_foot_ep_4  act_still_ep_4  \\\n",
      "0             0.000000          0.000000          0.000000    21600.000000   \n",
      "1          1434.500000         52.000000        211.000000    19785.000000   \n",
      "2           701.142857          3.000000        296.142857    19977.142857   \n",
      "3           957.666667         10.333333         57.000000    20424.666667   \n",
      "4           594.500000         52.500000       1321.500000    18899.500000   \n",
      "\n",
      "   act_tilting_ep_4  act_unknown_ep_4  audio_amp_mean_ep_4  \\\n",
      "0          0.000000          0.000000           371.575458   \n",
      "1         46.500000         71.000000          2924.937914   \n",
      "2        162.142857        460.428571          1687.404919   \n",
      "3         99.666667         50.666667          1907.264467   \n",
      "4        239.000000        493.000000          2144.723871   \n",
      "\n",
      "   audio_amp_std_ep_4  audio_convo_duration_ep_4  audio_convo_num_ep_4  ...  \\\n",
      "0           44.864366                   0.000000              0.000000  ...   \n",
      "1         2867.863586                6518.000000              4.500000  ...   \n",
      "2         1471.305708                6753.000000              5.571429  ...   \n",
      "3         1556.537067                3728.333333              7.333333  ...   \n",
      "4         1178.791993               10158.500000              6.500000  ...   \n",
      "\n",
      "   loc_dist_sum  loc_visit_num_sum  sms_in_num_sum  sms_out_num_sum  \\\n",
      "0      0.000000                0.0        0.000000              0.0   \n",
      "1  23784.117852                4.5        0.000000              0.0   \n",
      "2  64316.396522                7.0        0.142857              0.0   \n",
      "3  17195.431804                5.0        0.333333              0.0   \n",
      "4  47330.610429                5.0        0.000000              0.0   \n",
      "\n",
      "   unlock_duration_sum  unlock_num_sum  ema_score  study_id  start_day  \\\n",
      "0            10.790286        0.285714        6.0        -1   20150128   \n",
      "1           245.268000       10.000000        6.0        -1   20150204   \n",
      "2           522.720857       19.142857        5.0        -1   20150206   \n",
      "3           144.071333        7.000000       14.0        -1   20150213   \n",
      "4           375.040500       14.500000       13.0        -1   20150216   \n",
      "\n",
      "    end_day  \n",
      "0  20150204  \n",
      "1  20150206  \n",
      "2  20150213  \n",
      "3  20150216  \n",
      "4  20150218  \n",
      "\n",
      "[5 rows x 73 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove columns that have only one unique value\n",
    "data = data.loc[:, data.nunique() > 1]\n",
    "\n",
    "# Save the cleaned dataset\n",
    "#data.to_csv(\"processed_ema_dataset.csv\", index=False)\n",
    "\n",
    "#print(f\"âœ… Cleaned dataset saved with {data.shape[1]} features\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      act_in_vehicle_ep_4  act_on_bike_ep_4  act_on_foot_ep_4  act_still_ep_4  \\\n",
      "0                0.000000          0.000000          0.000000    21600.000000   \n",
      "1             1434.500000         52.000000        211.000000    19785.000000   \n",
      "2              701.142857          3.000000        296.142857    19977.142857   \n",
      "3              957.666667         10.333333         57.000000    20424.666667   \n",
      "4              594.500000         52.500000       1321.500000    18899.500000   \n",
      "...                   ...               ...               ...             ...   \n",
      "6358          3510.000000         36.666667        798.666667    14936.333333   \n",
      "6359          2012.500000         16.500000       1061.000000    16086.500000   \n",
      "6360          8275.000000         63.500000       1354.500000     7828.000000   \n",
      "6361          7850.666667         35.333333        752.000000     9019.333333   \n",
      "6362          8955.000000         24.000000        426.500000     7956.000000   \n",
      "\n",
      "      act_tilting_ep_4  act_unknown_ep_4  audio_amp_mean_ep_4  \\\n",
      "0             0.000000          0.000000           371.575458   \n",
      "1            46.500000         71.000000          2924.937914   \n",
      "2           162.142857        460.428571          1687.404919   \n",
      "3            99.666667         50.666667          1907.264467   \n",
      "4           239.000000        493.000000          2144.723871   \n",
      "...                ...               ...                  ...   \n",
      "6358        833.000000       1485.333333          2223.521163   \n",
      "6359        800.500000       1623.000000          1619.666911   \n",
      "6360       1883.500000       2195.500000          3330.949605   \n",
      "6361       1602.666667       2340.000000          3263.023111   \n",
      "6362       1869.500000       2369.000000          3794.645078   \n",
      "\n",
      "      audio_amp_std_ep_4  audio_convo_duration_ep_4  audio_convo_num_ep_4  \\\n",
      "0              44.864366                   0.000000              0.000000   \n",
      "1            2867.863586                6518.000000              4.500000   \n",
      "2            1471.305708                6753.000000              5.571429   \n",
      "3            1556.537067                3728.333333              7.333333   \n",
      "4            1178.791993               10158.500000              6.500000   \n",
      "...                  ...                        ...                   ...   \n",
      "6358         2541.317313                2023.666667              9.666667   \n",
      "6359         1724.866217               11159.000000             18.000000   \n",
      "6360         2406.027559                6971.000000             22.000000   \n",
      "6361         2602.283875                5295.333333             20.000000   \n",
      "6362         2835.939463                7725.000000             22.000000   \n",
      "\n",
      "      ...   loc_dist_sum  loc_visit_num_sum  sms_in_num_sum  sms_out_num_sum  \\\n",
      "0     ...       0.000000                0.0        0.000000         0.000000   \n",
      "1     ...   23784.117852                4.5        0.000000         0.000000   \n",
      "2     ...   64316.396522                7.0        0.142857         0.000000   \n",
      "3     ...   17195.431804                5.0        0.333333         0.000000   \n",
      "4     ...   47330.610429                5.0        0.000000         0.000000   \n",
      "...   ...            ...                ...             ...              ...   \n",
      "6358  ...  164539.972617                8.0       29.333333        18.333333   \n",
      "6359  ...   81179.391848                4.0       25.000000        21.500000   \n",
      "6360  ...  126373.218042                9.0       34.000000        37.500000   \n",
      "6361  ...  136478.071997                9.0       55.666667        69.666667   \n",
      "6362  ...  197941.148511                5.0       37.500000        43.500000   \n",
      "\n",
      "      unlock_duration_sum  unlock_num_sum  ema_score  study_id  start_day  \\\n",
      "0               10.790286        0.285714        6.0        -1   20150128   \n",
      "1              245.268000       10.000000        6.0        -1   20150204   \n",
      "2              522.720857       19.142857        5.0        -1   20150206   \n",
      "3              144.071333        7.000000       14.0        -1   20150213   \n",
      "4              375.040500       14.500000       13.0        -1   20150216   \n",
      "...                   ...             ...        ...       ...        ...   \n",
      "6358         49547.697333      438.000000       15.0        91   20170707   \n",
      "6359         66453.177500      320.000000        9.0        91   20170710   \n",
      "6360         49117.092500      355.500000        5.0        91   20170712   \n",
      "6361         70553.461000      388.666667        2.0        91   20170714   \n",
      "6362         82051.677000      284.500000        0.0        91   20170717   \n",
      "\n",
      "       end_day  \n",
      "0     20150204  \n",
      "1     20150206  \n",
      "2     20150213  \n",
      "3     20150216  \n",
      "4     20150218  \n",
      "...        ...  \n",
      "6358  20170710  \n",
      "6359  20170712  \n",
      "6360  20170714  \n",
      "6361  20170717  \n",
      "6362  20170719  \n",
      "\n",
      "[6363 rows x 73 columns]\n"
     ]
    }
   ],
   "source": [
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data split complete: 5090 training rows, 1273 test rows\n"
     ]
    }
   ],
   "source": [
    "# Features (X) and Target (y)\n",
    "X = data.drop(['ema_score', 'study_id', 'start_day', 'end_day'], axis=1)  # Drop non-relevant columns\n",
    "y = data['ema_score']\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"âœ… Data split complete: {X_train.shape[0]} training rows, {X_test.shape[0]} test rows\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to TensorFlow format\n",
    "X_train_tf = tf.convert_to_tensor(X_train_scaled, dtype=tf.float32)\n",
    "X_test_tf = tf.convert_to_tensor(X_test_scaled, dtype=tf.float32)\n",
    "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "# Check shape\n",
    "print(f\"Train shape: {X_train_tf.shape}, Test shape: {X_test_tf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the SVR model\n",
    "svm_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)  # Default hyperparameters\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,    # Number of trees (can increase for better performance)\n",
    "    max_depth=None,      # Let the model decide optimal depth\n",
    "    random_state=42,     # Ensures reproducibility\n",
    "    n_jobs=-1            # Utilizes all CPU cores for faster training\n",
    ")\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,     # Number of trees\n",
    "    learning_rate=0.1,    # Learning rate (controls step size)\n",
    "    max_depth=6,          # Tree depth to control overfitting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize the LightGBM model\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=200,  # Number of trees\n",
    "    learning_rate=0.1,  # Step size\n",
    "    max_depth=6,  # Limits tree depth to prevent overfitting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define the neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
    "    keras.layers.Dense(64, activation='relu'),  # Hidden layer 1\n",
    "    keras.layers.Dense(32, activation='relu'),  # Hidden layer 2\n",
    "    keras.layers.Dense(1)  # Output layer (regression)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SVM Model Performance:\n",
      "ðŸ“‰ MSE: 34.73\n",
      "ðŸ“ˆ RÂ²: -0.01\n",
      "ðŸŽ¯ Accuracy: 36.95%\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "# Custom Regression Accuracy Function\n",
    "def regression_accuracy(y_true, y_pred):\n",
    "    return 100 * (1 - (mean_absolute_error(y_true, y_pred) / y_true.mean()))\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "accuracy = regression_accuracy(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… SVM Model Performance:\")\n",
    "print(f\"ðŸ“‰ MSE: {mse:.2f}\")\n",
    "print(f\"ðŸ“ˆ RÂ²: {r2:.2f}\")\n",
    "print(f\"ðŸŽ¯ Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Random Forest Model Performance:\n",
      "ðŸ“‰ MSE: 10.24\n",
      "ðŸ“ˆ RÂ²: 0.70\n",
      "ðŸŽ¯ Accuracy: 70.10%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "accuracy = max(0, 1 - (mse / np.var(y_test)))\n",
    "\n",
    "print(f\"âœ… Random Forest Model Performance:\")\n",
    "print(f\"ðŸ“‰ MSE: {mse:.2f}\")\n",
    "print(f\"ðŸ“ˆ RÂ²: {r2:.2f}\")\n",
    "print(f\"ðŸŽ¯ Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost Model Performance:\n",
      "ðŸ“‰ MSE: 10.15\n",
      "ðŸ“ˆ RÂ²: 0.70\n",
      "ðŸŽ¯ Accuracy: 70.34%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "accuracy = max(0, 1 - (mse / np.var(y_test)))  # Regression accuracy approximation\n",
    "\n",
    "print(f\"âœ… XGBoost Model Performance:\")\n",
    "print(f\"ðŸ“‰ MSE: {mse:.2f}\")\n",
    "print(f\"ðŸ“ˆ RÂ²: {r2:.2f}\")\n",
    "print(f\"ðŸŽ¯ Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12591\n",
      "[LightGBM] [Info] Number of data points in the train set: 5090, number of used features: 67\n",
      "[LightGBM] [Info] Start training from score 7.680157\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "âœ… LightGBM Model Performance:\n",
      "ðŸ“‰ MSE: 10.38\n",
      "ðŸ“ˆ RÂ²: 0.70\n",
      "ðŸŽ¯ Accuracy: 69.68%\n"
     ]
    }
   ],
   "source": [
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "accuracy = max(0, 1 - (mse / np.var(y_test)))  # Regression accuracy approximation\n",
    "\n",
    "print(f\"âœ… LightGBM Model Performance:\")\n",
    "print(f\"ðŸ“‰ MSE: {mse:.2f}\")\n",
    "print(f\"ðŸ“ˆ RÂ²: {r2:.2f}\")\n",
    "print(f\"ðŸŽ¯ Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"âœ… Neural Network Performance:\")\n",
    "print(f\"ðŸ“‰ Loss (MSE): {loss:.2f}\")\n",
    "print(f\"ðŸ“ˆ MAE: {mae:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
