{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (23573, 155)\n",
      "Column Names: Index(['study_id', 'eureka_id', 'day', 'act_in_vehicle_ep_0',\n",
      "       'act_in_vehicle_ep_1', 'act_in_vehicle_ep_2', 'act_in_vehicle_ep_3',\n",
      "       'act_in_vehicle_ep_4', 'act_on_bike_ep_0', 'act_on_bike_ep_1',\n",
      "       ...\n",
      "       'unlock_duration_ep_0', 'unlock_duration_ep_1', 'unlock_duration_ep_2',\n",
      "       'unlock_duration_ep_3', 'unlock_duration_ep_4', 'unlock_num_ep_0',\n",
      "       'unlock_num_ep_1', 'unlock_num_ep_2', 'unlock_num_ep_3',\n",
      "       'unlock_num_ep_4'],\n",
      "      dtype='object', length=155)\n",
      "   study_id eureka_id       day  act_in_vehicle_ep_0  act_in_vehicle_ep_1  \\\n",
      "0        -1      u004  20150122                    0                    0   \n",
      "1        -1      u004  20150123                    0                    0   \n",
      "2        -1      u004  20150124                    0                    0   \n",
      "3        -1      u004  20150125                    0                    0   \n",
      "4        -1      u004  20150126                    0                    0   \n",
      "\n",
      "   act_in_vehicle_ep_2  act_in_vehicle_ep_3  act_in_vehicle_ep_4  \\\n",
      "0                    0                    0                    0   \n",
      "1                    0                    0                    0   \n",
      "2                    0                    0                    0   \n",
      "3                    0                    0                    0   \n",
      "4                    0                    0                    0   \n",
      "\n",
      "   act_on_bike_ep_0  act_on_bike_ep_1  ...  unlock_duration_ep_0  \\\n",
      "0                 0                 0  ...                   0.0   \n",
      "1                 0                 0  ...                   0.0   \n",
      "2                 0                 0  ...                   0.0   \n",
      "3                 0                 0  ...                   0.0   \n",
      "4                 0                 0  ...                   0.0   \n",
      "\n",
      "   unlock_duration_ep_1  unlock_duration_ep_2  unlock_duration_ep_3  \\\n",
      "0                   0.0                   0.0                   0.0   \n",
      "1                   0.0                   0.0                   0.0   \n",
      "2                   0.0                   0.0                   0.0   \n",
      "3                   0.0                   0.0                   0.0   \n",
      "4                   0.0                   0.0                   0.0   \n",
      "\n",
      "   unlock_duration_ep_4  unlock_num_ep_0  unlock_num_ep_1  unlock_num_ep_2  \\\n",
      "0                   0.0                0                0                0   \n",
      "1                   0.0                0                0                0   \n",
      "2                   0.0                0                0                0   \n",
      "3                   0.0                0                0                0   \n",
      "4                   0.0                0                0                0   \n",
      "\n",
      "   unlock_num_ep_3  unlock_num_ep_4  \n",
      "0                0                0  \n",
      "1                0                0  \n",
      "2                0                0  \n",
      "3                0                0  \n",
      "4                0                0  \n",
      "\n",
      "[5 rows x 155 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'CrossCheck_Daily_Data.xlsx'  \n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"Column Names:\", data.columns)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the eps to make it one full day\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "new_columns = []\n",
    "processed_base_columns = set()  \n",
    "columns_to_drop = []  \n",
    "\n",
    "\n",
    "epochs = ['ep_0', 'ep_1', 'ep_2', 'ep_3']  #only taking the first four eps as they indicates the whole day\n",
    "\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if any(epoch in col for epoch in epochs):  # to see if the coulmn has ep_ in it \n",
    "        \n",
    "        #removing ep for the bas coulmn name\n",
    "        epoch_num = [epoch for epoch in epochs if epoch in col][0]\n",
    "        base_col_name = col.replace(f'_{epoch_num}', '') \n",
    "\n",
    "        # Check if the coulmn has been aggregated before and not aggregate ep_4 \n",
    "        if base_col_name not in processed_base_columns:\n",
    "            columns_to_sum = [col for col in data.columns if base_col_name in col and 'ep_4' not in col]\n",
    "            aggregated_col = data[columns_to_sum].sum(axis=1)\n",
    "            new_columns.append(aggregated_col.rename(base_col_name + '_sum'))\n",
    "            \n",
    "\n",
    "            columns_to_drop.extend([col for col in columns_to_sum if 'ep_4' not in col]) \n",
    "\n",
    "            # Mark the base column as processed\n",
    "            processed_base_columns.add(base_col_name)\n",
    "\n",
    "# add the new coulmns to the data \n",
    "if new_columns:\n",
    "    data = pd.concat([data] + new_columns, axis=1)\n",
    "\n",
    "#drop the original coulmns \n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "#data.to_csv(\"processed_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Identify rows with recorded EMA scores\n",
    "data['is_target'] = data['ema_score'].notna()\n",
    "\n",
    "# Find the indices where EMA scores are available\n",
    "ema_indices = data[data['is_target']].index.tolist()\n",
    "\n",
    "# List to store new dataset rows\n",
    "new_data = []\n",
    "\n",
    "# Extract feature columns (excluding 'ema_score', 'study_id', 'eureka_id', and 'day')\n",
    "feature_cols = [col for col in data.columns if col not in ['ema_score', 'study_id', 'eureka_id', 'day', 'is_target']]\n",
    "\n",
    "# Iterate over each EMA segment\n",
    "for i in range(len(ema_indices) - 1):\n",
    "    start_idx = ema_indices[i]  # Start from last recorded EMA\n",
    "    end_idx = ema_indices[i + 1]  # Stop at next recorded EMA\n",
    "\n",
    "    # Get segment data\n",
    "    segment = data.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Compute average of feature columns\n",
    "    avg_features = segment[feature_cols].mean().to_dict()\n",
    "\n",
    "    # Assign the next recorded EMA score as the target\n",
    "    avg_features['ema_score'] = data.loc[end_idx, 'ema_score']\n",
    "\n",
    "    # Store metadata\n",
    "    avg_features['study_id'] = data.loc[start_idx, 'study_id']  # Keep the participant ID\n",
    "    avg_features['start_day'] = data.loc[start_idx, 'day']  # First day in the segment\n",
    "    avg_features['end_day'] = data.loc[end_idx, 'day']  # Last day in the segment\n",
    "\n",
    "    # Store the result\n",
    "    new_data.append(avg_features)\n",
    "\n",
    "# Convert new dataset to DataFrame\n",
    "data = pd.DataFrame(new_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Remove columns that have only one unique value\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m[:, new_data\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the cleaned dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m new_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_ema_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "# Remove columns that have only one unique value\n",
    "data = data.loc[:, data.nunique() > 1]\n",
    "\n",
    "# Save the cleaned dataset\n",
    "data.to_csv(\"processed_ema_dataset.csv\", index=False)\n",
    "\n",
    "#print(f\"âœ… Cleaned dataset saved with {data.shape[1]} features\")\n",
    "#print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['start_day', 'end_day'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Features (X) and Target (y)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mema_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstudy_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_day\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend_day\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Drop non-relevant columns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train-test split (80% train, 20% test)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['start_day', 'end_day'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Features (X) and Target (y)\n",
    "X = new_data.drop(['ema_score', 'study_id', 'start_day', 'end_day'], axis=1)  # Drop non-relevant columns\n",
    "y = new_data['ema_score']\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"âœ… Data split complete: {X_train.shape[0]} training rows, {X_test.shape[0]} test rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the SVR model\n",
    "svm_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)  # Default hyperparameters\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… SVM Model Performance:\")\n",
    "print(f\"ğŸ“‰ MSE: {mse:.2f}\")\n",
    "print(f\"ğŸ“ˆ RÂ²: {r2:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
